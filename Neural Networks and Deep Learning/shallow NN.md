This is the 3rd week of first course, in this week, the learning objective is to : Learn to build a neural network with one hidden layer, using forward propagation and backpropagation. 
The main content of this week as follows:
- [Neural Networks Overview](#neural-networks-overview)
- [Neural Networks Representation](#neural-networks-representation)
- [Computing a Neural Network's output](#computing-a-neural-network's-output)
- [Vectorizing across multiple examples](#vectorizing-across-multiple-examples)
- [Explanation for Vectorized Implementation](#explanation-for-vectorized-implementation)
- [Activation Function](#activation-function)
- [Why do we need non-linear activation functions](#why-do-we-need-non-linear-activation-function)
- [Derivatives of activation functions](#derivatives-of-activation-functions)
- [Gradient Descent for Neural Networks](#gradient-descent-for-neural-networks)
- [Backpropagation Intuition](#backpropagation-ntuition)
- [Random Initialization](#random-initialization)
## Neural Networks overview
From previous study, a simple logistic regression can be represented as:

`Z=W*X+b A=sigmoid(Z)`

![](images/logistic.png)

A Neural Network with 1 hidden layer can be represented as (repeat logistic regression once):

`Z[1]=W[1]*X+b[1] A[1]=sigmoid(Z[1]) Z[2]=W[2]*A[1]+b[2] A[2]=sigmoid(Z[2])`
![](images/1nn.png)
